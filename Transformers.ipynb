{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjncuDf2qugI"
      },
      "source": [
        "## **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9VSf2D_F5iU"
      },
      "source": [
        "### Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dynamic landscape of the media and news industry, the ability to swiftly categorize and curate content has become a strategic imperative. The vast volume of information demands efficient systems to organize and present content to the audience.\n",
        "\n",
        "The media industry, being the pulse of information dissemination, grapples with the continuous influx of news articles spanning diverse topics. Ensuring that the right articles reach the right audience promptly is not just a logistical necessity but a critical component in retaining and engaging audiences in an age of information overload.\n",
        "\n",
        "Common Industry Challenges:\n",
        "Amidst the ceaseless flow of news, organizations encounter challenges such as:\n",
        "- Information Overload: The sheer volume of news articles makes manual categorization impractical.\n",
        "- Timeliness: Delays in categorizing news articles can result in outdated or misplaced content."
      ],
      "metadata": {
        "id": "RLApwDNbFDEK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzSKXh2LsOvd"
      },
      "source": [
        "### Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E-news Express, a news aggregation startup, faces the challenge of categorizing the news articles collected. With news articles covering sports, entertainment, politics, and more, the need for an advanced and automated system to categorize them has become increasingly evident. The manual efforts required for categorizing such a diverse range of news articles are substantial, and human errors in the categorization of news articles can lead to reputational damage for the startup. There is also the factor of delays and potential inaccuracies. To streamline and optimize this process, the organization recognizes the imperative of adopting cutting-edge technologies, particularly machine learning, to automate and enhance the categorization of content.\n",
        "\n",
        "As a data scientist on the E-news Express data team, the task is to analyze the text in news articles and build an unsupervised learning model for categorizing them. The categorization done by the model can then be validated against human-defined labels to check the overall accuracy of the AI system. The goal is to optimize the categorization process, ensuring timely and personalized delivery."
      ],
      "metadata": {
        "id": "PdEWl1qZFNWM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saFx1pbT_zTP"
      },
      "source": [
        "### Data Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Text**: The main body of the news article"
      ],
      "metadata": {
        "id": "5rJpifbLGVDe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svwbW6qDUB_B"
      },
      "source": [
        "## **Installing and importing the necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvO3P-BZT-ZV"
      },
      "outputs": [],
      "source": [
        "# installing the sentence-transformers library\n",
        "!pip install -U sentence-transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to read and manipulate the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('max_colwidth', None)    # setting column to the maximum column width as per the data\n",
        "\n",
        "# to visualise data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# to compute distances\n",
        "from scipy.spatial.distance import cdist, pdist\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# importing the PyTorch Deep Learning library\n",
        "import torch\n",
        "\n",
        "# to import the model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# to cluster the data\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# to compute metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# to avoid displaying unnecessary warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "47oRo-kTaR02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-PA0a0AUnrR"
      },
      "source": [
        "## **Loading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "ZVWbBVQaKwUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7VtUDiSUp0C"
      },
      "outputs": [],
      "source": [
        "reviews = pd.read_csv(\"/content/drive/MyDrive/AIMLTraining/NLP/news_articles_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a copy of the dataset\n",
        "data = reviews.copy()"
      ],
      "metadata": {
        "id": "482415eWbJ94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvlzvKeqAH-i"
      },
      "source": [
        "## **Data Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIH4md8nAL4v"
      },
      "source": [
        "### Checking the first five rows of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKS-Z7GiCmWX"
      },
      "outputs": [],
      "source": [
        "# Print first 5 rows of data\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTh2uQdWwY9-"
      },
      "outputs": [],
      "source": [
        "# checking a news article\n",
        "data.loc[3, 'Text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu_WH-DElvdg"
      },
      "source": [
        "### Checking the last five rows of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJwhDJt0lvdm"
      },
      "outputs": [],
      "source": [
        "# Print last 5 rows of data\n",
        "data.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuWYF7W_AQx_"
      },
      "source": [
        "### Checking the shape of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DysT_j1Cky-"
      },
      "outputs": [],
      "source": [
        "# print shape of data\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m9Eku_JUDaS"
      },
      "source": [
        "* The data comprises of ~2.2k news articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBuO6NvsAT1k"
      },
      "source": [
        "### Checking for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjFc0JaDCn1u"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9JV6etiY0g9"
      },
      "source": [
        "- There are no mising values in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8MHLoYFk4cj"
      },
      "source": [
        "### Checking for duplicate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0riz6vzzk4cu"
      },
      "outputs": [],
      "source": [
        "# Check for duplicate values\n",
        "data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIaC9FQPk4cv"
      },
      "source": [
        "- We'll drop the duplicate values in the data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop_duplicates()\n",
        "\n",
        "# resetting the dataframe index\n",
        "data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "qbk7cAnTk8FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "1ivDvrO7lF0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are no duplicate values in the data now."
      ],
      "metadata": {
        "id": "hZZHuFYZlHXi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYtRRwpJKTxz"
      },
      "source": [
        "## **Model Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Defining the model**"
      ],
      "metadata": {
        "id": "51ITQezWi9VE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNDAvKn0Klwa"
      },
      "source": [
        "We'll be using the **all-MiniLM-L6-v2** model here.\n",
        "\n",
        "üí° The **all-MiniLM-L6-v2** model is an all-round (**all**) model trained on a large and diverse dataset of over 1 billion training samples and generates state-of-the-art sentence embeddings of 384 dimensions.\n",
        "\n",
        "üìä  It is a language model (**LM**) that has 6 transformer encoder layers (**L6**) and is a smaller model (**Mini**) trained to mimic the performance of a larger model (BERT).\n",
        "\n",
        "üõ†Ô∏è Potential use-cases include text classification, sentiment analysis, and semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EQ7eQIpYSyz"
      },
      "outputs": [],
      "source": [
        "#Defining the model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lll4MLfzKfBa"
      },
      "source": [
        "### **Encoding the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "wqQUCHjwcN6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1BaGKThKcX3"
      },
      "outputs": [],
      "source": [
        "# encoding the dataset\n",
        "embedding_matrix = model.encode(data['Text'], show_progress_bar=True, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oqi4c8sBC9pH"
      },
      "outputs": [],
      "source": [
        "# printing the shape of the embedding matrix\n",
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Each news article has been converted to a 384-dimensional vector"
      ],
      "metadata": {
        "id": "hcGWEfJnBmQ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi3-HSa7GghW"
      },
      "outputs": [],
      "source": [
        "# printing the embedding vector of the first review in the dataset\n",
        "embedding_matrix[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: We have not trained or fine-tuned the model. We have used the pre-trained model to encode the dataset."
      ],
      "metadata": {
        "id": "LH3_pyqajPtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Semantic Search**"
      ],
      "metadata": {
        "id": "-WmQe5LKivez"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvoEaOWRKkqu"
      },
      "outputs": [],
      "source": [
        "# defining a function to compute the cosine similarity between two embedding vectors\n",
        "def cosine_score(text):\n",
        "    # encoding the text\n",
        "    embeddings = model.encode(text)\n",
        "\n",
        "    # calculating the L2 norm of the embedding vector\n",
        "    norm1 = np.linalg.norm(embeddings[0])\n",
        "    norm2 = np.linalg.norm(embeddings[1])\n",
        "\n",
        "    # computing the cosine similarity\n",
        "    cosine_similarity_score = ((np.dot(embeddings[0],embeddings[1]))/(norm1*norm2))\n",
        "\n",
        "    return cosine_similarity_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxiB0ewYNVS8"
      },
      "source": [
        "**Now, let's search for similar reviews in our dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSjjKSc-OFvj"
      },
      "outputs": [],
      "source": [
        "# defining a function to find the top k similar sentences for a given query\n",
        "def top_k_similar_sentences(embedding_matrix, query_text, k):\n",
        "    # encoding the query text\n",
        "    query_embedding = model.encode(query_text)\n",
        "\n",
        "    # calculating the cosine similarity between the query vector and all other encoded vectors of our dataset\n",
        "    score_vector = np.dot(embedding_matrix,query_embedding)\n",
        "\n",
        "    # sorting the scores in descending order and choosing the first k\n",
        "    top_k_indices = np.argsort(score_vector)[::-1][:k]\n",
        "\n",
        "\n",
        "    # returning the corresponding reviews\n",
        "    return data.loc[list(top_k_indices), 'Text']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the query text\n",
        "query_text = \"Budget for elections\"\n",
        "\n",
        "# displaying the top 5 similar sentences\n",
        "top_k_reviews = top_k_similar_sentences(embedding_matrix, query_text, 5)\n",
        "\n",
        "for i in top_k_reviews:\n",
        "    print(i, end=\"\\n\\n\")"
      ],
      "metadata": {
        "id": "XsrLRlSVjbuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the query text\n",
        "query_text = \"High imports and exports\"\n",
        "\n",
        "# displaying the top 5 similar sentences\n",
        "top_k_reviews = top_k_similar_sentences(embedding_matrix, query_text, 5)\n",
        "\n",
        "for i in top_k_reviews:\n",
        "    print(i, end=\"\\n\\n\")"
      ],
      "metadata": {
        "id": "L4coZrXxmL9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Categorization**"
      ],
      "metadata": {
        "id": "7Dda9VmGGDXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We'll use K-Means Clustering to categorize the data.**"
      ],
      "metadata": {
        "id": "iMXq_FK2mp2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meanDistortions = []\n",
        "clusters = range(2, 11)\n",
        "\n",
        "for k in clusters:\n",
        "    clusterer = KMeans(n_clusters=k, random_state=1)\n",
        "    clusterer.fit(embedding_matrix)\n",
        "\n",
        "    prediction = clusterer.predict(embedding_matrix)\n",
        "\n",
        "    distortion = sum(\n",
        "        np.min(cdist(embedding_matrix, clusterer.cluster_centers_, \"euclidean\"), axis=1) ** 2\n",
        "    )\n",
        "    meanDistortions.append(distortion)\n",
        "\n",
        "    print(\"Number of Clusters:\", k, \"\\tAverage Distortion:\", distortion)"
      ],
      "metadata": {
        "id": "HWusI5e1B4dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(clusters, meanDistortions, \"bx-\")\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Average Distortion\")\n",
        "plt.title(\"Selecting k with the Elbow Method\", fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "--y7gXzWCPt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The  value of k from the Elbow curve we will consider is 5"
      ],
      "metadata": {
        "id": "h6yvWk6fDIY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's check the silhouette scores.**"
      ],
      "metadata": {
        "id": "ysM1Pl1cCWYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sil_score = []\n",
        "cluster_list = range(2, 10)\n",
        "\n",
        "for n_clusters in cluster_list:\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=1)\n",
        "\n",
        "    preds = clusterer.fit_predict((embedding_matrix))\n",
        "\n",
        "    score = silhouette_score(embedding_matrix, preds)\n",
        "    sil_score.append(score)\n",
        "\n",
        "    print(\"For n_clusters = {}, the silhouette score is {})\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "J-895NpSCXXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cluster_list, sil_score, \"bx-\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XIzCjafZCfOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The silhouette coefficient for 5 clusters is the highest.\n",
        "- So, we will proceed with 5 clusters."
      ],
      "metadata": {
        "id": "P5gFa2nHDHKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOJI7_6FmFNb"
      },
      "outputs": [],
      "source": [
        "# defining the number of clusters/categories\n",
        "n_categories = 5\n",
        "\n",
        "# fitting the model\n",
        "kmeans = KMeans(n_clusters=n_categories, random_state=1).fit(embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6N-d-pImTPP"
      },
      "outputs": [],
      "source": [
        "# checking the cluster centers\n",
        "centers = kmeans.cluster_centers_\n",
        "centers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a copy of the data\n",
        "clustered_data = data.copy()\n",
        "\n",
        "# assigning the cluster/category labels\n",
        "clustered_data['Category'] = kmeans.labels_\n",
        "\n",
        "clustered_data.head()"
      ],
      "metadata": {
        "id": "-kntIukTnqSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check a few random news articles from each of the categories."
      ],
      "metadata": {
        "id": "xK3ThY3OoF6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for each cluster, printing the 5 random news articles\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "    print(clustered_data.loc[clustered_data.Category == i, 'Text'].sample(5, random_state=1).values)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "DB9IZ39vD5Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above news articles, we can see that they can be categorized as follows:\n",
        "\n",
        "- 0: Technology\n",
        "- 1: Sports\n",
        "- 2: Politics\n",
        "- 3: Entertainment\n",
        "- 4: Business"
      ],
      "metadata": {
        "id": "FH9piRxZXJqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary of cluster label to category\n",
        "category_dict = {\n",
        "     0: 'Technology',\n",
        "     1: 'Politics',\n",
        "     2: 'Sports',\n",
        "     3: 'Business',\n",
        "     4: 'Entertainment'\n",
        "}\n",
        "\n",
        "# mapping cluster labels to categories\n",
        "clustered_data['Category'] = clustered_data['Category'].map(category_dict)\n",
        "\n",
        "clustered_data.head()"
      ],
      "metadata": {
        "id": "_bEUw-9dsAn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hm4-zm4j8vb"
      },
      "source": [
        "## **Comparing with Actual Categories**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the actual labels\n",
        "labels = pd.read_csv(\"/content/drive/MyDrive/AIMLTraining/NLP/news_article_labels.csv\")"
      ],
      "metadata": {
        "id": "8FdsQ9OOGSZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape"
      ],
      "metadata": {
        "id": "GMRasDc_y9G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the unique labels\n",
        "labels['Label'].unique()"
      ],
      "metadata": {
        "id": "71GVALsl0CLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding the actual categories to our dataframe\n",
        "clustered_data['Actual Category'] = labels['Label'].values"
      ],
      "metadata": {
        "id": "PKS4zyom5kaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(clustered_data['Actual Category'], clustered_data['Category']))"
      ],
      "metadata": {
        "id": "FmJOUJHy5qYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We were able to categorize the news articles with 96% accuracy\n",
        "\n",
        "- Note that we were able to do so WITHOUT knowing the actual labels for the news articles."
      ],
      "metadata": {
        "id": "_E7KBIF35JzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analyzing Incorrect Predictions**"
      ],
      "metadata": {
        "id": "xP5x5M5a6kg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check a few cases where our model incorrectly categorized the news articles."
      ],
      "metadata": {
        "id": "YwGLhbn65Y6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a dataframe of incorrect categorizations\n",
        "incorrect_category_data = clustered_data[clustered_data['Actual Category'] != clustered_data['Category']].copy()\n",
        "incorrect_category_data.shape"
      ],
      "metadata": {
        "id": "D6hxwjCX5gYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect_category_data.head()"
      ],
      "metadata": {
        "id": "wXDidNvI6KWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 24\n",
        "\n",
        "print('Distance from Actual Category')\n",
        "print(cdist(embedding_matrix[idx].reshape(1,-1), kmeans.cluster_centers_[[2]], \"euclidean\")[0,0])\n",
        "\n",
        "print('Distance from Predicted Category')\n",
        "print(cdist(embedding_matrix[idx].reshape(1,-1), kmeans.cluster_centers_[[3]], \"euclidean\")[0,0])"
      ],
      "metadata": {
        "id": "ik58-0BV9LTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the distance of the particular news article from the cluster centers of the actual and predicted categories is almost similar."
      ],
      "metadata": {
        "id": "5U6ZfbaB9Xev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 45\n",
        "\n",
        "print('Distance from Actual Category')\n",
        "print(cdist(embedding_matrix[idx].reshape(1,-1), kmeans.cluster_centers_[[2]], \"euclidean\")[0,0])\n",
        "\n",
        "print('Distance from Predicted Category')\n",
        "print(cdist(embedding_matrix[idx].reshape(1,-1), kmeans.cluster_centers_[[4]], \"euclidean\")[0,0])"
      ],
      "metadata": {
        "id": "h0pO4Fui8GvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the distance of the particular news article from the cluster centers of the actual and predicted categories is almost similar."
      ],
      "metadata": {
        "id": "efuxIYIf9xK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on this, we can say that a better approach of categorizing these news articles would be to assign more than one category to these news article."
      ],
      "metadata": {
        "id": "Oqf6MjBN9zZw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH77U4ukBiCe"
      },
      "source": [
        "## **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We did the following in the case study:\n",
        "\n",
        "1. Encoded the dataset using the ***all-MiniLM-L6-v2*** transformer model to generate embeddings of 384 dimensions\n",
        "\n",
        "2. Queried the dataset to find news articles similar to the query text we passed\n",
        "\n",
        "3. Categorized the news articles using K-Means Clustering on the transformer encodings\n",
        "\n",
        "4. Compared the predicted categories of the news articles to the actual categories\n",
        "\n",
        "5. Analyzed the incorrect predictions to understand where the model went wrong"
      ],
      "metadata": {
        "id": "t0DuBIcvRsuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Our model can correctly categorize 96% of the news articles.\n",
        "\n",
        "- As mentioned, one can try tagging news articles with more than one category for better categorization.\n",
        "\n",
        "    - One can find the cluster centers to which the news article is the closest and assign one or more categories accordingly\n",
        "\n",
        "- Another approach that can be tried out would be fine-tuning the model to this particular data with category labels (one or more than one) to try and improve the overall performance.\n",
        "\n",
        "- In addition, the startup can use other transformer models to generate summaries of the news articles, which can provide a gist of the news content."
      ],
      "metadata": {
        "id": "v1PZyy_HABht"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "MjncuDf2qugI",
        "x9VSf2D_F5iU",
        "MzSKXh2LsOvd",
        "saFx1pbT_zTP",
        "svwbW6qDUB_B",
        "N-PA0a0AUnrR",
        "vvlzvKeqAH-i",
        "cIH4md8nAL4v",
        "xu_WH-DElvdg",
        "NuWYF7W_AQx_",
        "EBuO6NvsAT1k",
        "i8MHLoYFk4cj",
        "fYtRRwpJKTxz",
        "51ITQezWi9VE",
        "Lll4MLfzKfBa",
        "-WmQe5LKivez",
        "3hm4-zm4j8vb",
        "xP5x5M5a6kg1",
        "wH77U4ukBiCe"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}